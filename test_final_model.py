import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

def test_both_models():
    """æµ‹è¯•åŸå§‹æ¨¡å‹å’Œå¾®è°ƒåçš„æ¨¡å‹ï¼Œè¿›è¡Œå¯¹æ¯”"""
    
    print("=" * 80)
    print("å¤§æ¨¡å‹å¾®è°ƒæ•ˆæœå¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    test_questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
        "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹ç›‘ç£å¾®è°ƒ(SFT)",
        "DPOå’ŒSFTæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯RAGï¼Ÿ"
    ]
    
    # æµ‹è¯•åŸå§‹æ¨¡å‹
    print("\nğŸ”µ æµ‹è¯•åŸå§‹æ¨¡å‹ (Qwen2.5-1.5B)...")
    try:
        original_model_path = "Qwen/Qwen2.5-1.5B"
        tokenizer = AutoTokenizer.from_pretrained(original_model_path)
        model = AutoModelForCausalLM.from_pretrained(
            original_model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n{i}. ç”¨æˆ·: {question}")
            messages = [{"role": "user", "content": question}]
            text = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            inputs = tokenizer(text, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            print(f"   åŸå§‹æ¨¡å‹: {response}")
            
    except Exception as e:
        print(f"âŒ åŸå§‹æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "=" * 60)
    
    # æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
    print("\nğŸŸ¢ æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")
    try:
        tuned_model_path = "./output/my_course_sft"
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(tuned_model_path):
            print(f"âŒ å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {tuned_model_path}")
            return
        
        print(f"âœ… æ‰¾åˆ°å¾®è°ƒæ¨¡å‹è·¯å¾„: {tuned_model_path}")
        
        # é‡æ–°åŠ è½½tokenizerå’Œæ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(tuned_model_path)
        model = AutoModelForCausalLM.from_pretrained(
            tuned_model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n{i}. ç”¨æˆ·: {question}")
            messages = [{"role": "user", "content": question}]
            text = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            inputs = tokenizer(text, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            print(f"   å¾®è°ƒæ¨¡å‹: {response}")
            
    except Exception as e:
        print(f"âŒ å¾®è°ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼å¯ä»¥åœ¨ä¸Šé¢çš„è¾“å‡ºä¸­å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹çš„æ•ˆæœå·®å¼‚ã€‚")

if __name__ == "__main__":
    test_both_models()
