#!/usr/bin/env python3
"""
ç»ˆæè§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ä¸LLaMA-Factoryç›¸åŒçš„å¾®è°ƒæ–¹æ³•
ä½†å®Œå…¨ç»•è¿‡å…¶å¤æ‚çš„ä¾èµ–é“¾
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
import json
from datetime import datetime

print("ğŸ¤– å¼€å§‹å¤§æ¨¡å‹å¾®è°ƒ - ç»ˆæè§£å†³æ–¹æ¡ˆ")
print(f"å¼€å§‹æ—¶é—´: {datetime.now()}")
print("=" * 50)

# 1. ç¯å¢ƒéªŒè¯
print("1. éªŒè¯ç¯å¢ƒ...")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")

# 2. åŠ è½½æ¨¡å‹
print("2. åŠ è½½æ¨¡å‹...")
model_name = "Qwen/Qwen2-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    device_map="auto"
)
print(f"  æ¨¡å‹: {model_name}")
print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# 3. é…ç½®LoRAï¼ˆä½¿ç”¨LLaMA-Factoryçš„é»˜è®¤å‚æ•°ï¼‰
print("3. é…ç½®LoRA...")
lora_config = LoraConfig(
    r=8,           # LLaMA-Factoryé»˜è®¤
    lora_alpha=32, # LLaMA-Factoryé»˜è®¤
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)
print("  å¯è®­ç»ƒå‚æ•°:")
model.print_trainable_parameters()

# 4. å‡†å¤‡è®­ç»ƒæ•°æ®
print("4. å‡†å¤‡è®­ç»ƒæ•°æ®...")
train_data = [
    {"instruction": "è§£é‡Šæœºå™¨å­¦ä¹ ", "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿé€šè¿‡ç»éªŒè‡ªåŠ¨æ”¹è¿›æ€§èƒ½ã€‚"},
    {"instruction": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œ", "output": "ç¥ç»ç½‘ç»œæ˜¯å—äººè„‘å¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±ç›¸äº’è¿æ¥çš„ç¥ç»å…ƒå±‚ç»„æˆã€‚"},
    {"instruction": "è§£é‡Šè¿‡æ‹Ÿåˆ", "output": "è¿‡æ‹Ÿåˆæ˜¯æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å·®çš„ç°è±¡ã€‚"},
    {"instruction": "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«", "output": "ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡ç­¾æ•°æ®ï¼Œæ— ç›‘ç£å­¦ä¹ ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®ã€‚"},
    {"instruction": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ", "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå­¦ä¹ æ•°æ®è¡¨å¾ã€‚"}
]
print(f"  è®­ç»ƒæ ·æœ¬: {len(train_data)} æ¡")

# 5. æ•°æ®æ ¼å¼åŒ–
def format_instruction(example):
    return f"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['output']}"

formatted_texts = [format_instruction(ex) for ex in train_data]

# 6. Tokenization - ä¿®å¤ç‰ˆæœ¬
def tokenize_function(texts):
    encodings = []
    for text in texts:
        encoded = tokenizer(
            text,
            truncation=True,
            max_length=512,
            padding=False,
            return_tensors="pt"  # å…³é”®ä¿®æ”¹ï¼šè¿”å›PyTorchå¼ é‡
        )
        encodings.append({
            "input_ids": encoded["input_ids"].squeeze(0),
            "attention_mask": encoded["attention_mask"].squeeze(0),
            "labels": encoded["input_ids"].squeeze(0)
        })
    return encodings

tokenized_data = tokenize_function(formatted_texts)

# 7. è®­ç»ƒé…ç½®ï¼ˆä½¿ç”¨LLaMA-Factoryçš„é»˜è®¤å‚æ•°ï¼‰
print("5. é…ç½®è®­ç»ƒå‚æ•°...")
training_args = TrainingArguments(
    output_dir="./outputs/ultimate_solution",
    per_device_train_batch_size=1,      # LLaMA-Factoryé»˜è®¤
    gradient_accumulation_steps=8,       # LLaMA-Factoryé»˜è®¤
    learning_rate=1e-4,                  # LLaMA-Factoryé»˜è®¤
    num_train_epochs=3,                  # è®­ç»ƒè½®æ•°
    logging_steps=5,
    save_steps=50,
    fp16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False,
)

# 8. åˆ›å»ºTrainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data,
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    ),
)

# 9. å¼€å§‹è®­ç»ƒ
print("6. å¼€å§‹è®­ç»ƒ...")
print("=" * 50)
trainer.train()

# 10. ä¿å­˜ç»“æœ
trainer.save_model()
tokenizer.save_pretrained("./outputs/ultimate_solution")

print("=" * 50)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
print(f"ğŸ“ æ¨¡å‹ä¿å­˜è‡³: ./outputs/ultimate_solution")
print(f"â° å®Œæˆæ—¶é—´: {datetime.now()}")
print("=" * 50)
print("ä¸‹ä¸€æ­¥: æµ‹è¯•å¾®è°ƒæ¨¡å‹å¹¶å‡†å¤‡ä½œä¸šæŠ¥å‘Š")