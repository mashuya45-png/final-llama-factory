import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import os


class ModelDemo:
    def __init__(self):
        self.tuned_model = None
        self.tuned_tokenizer = None
        self.original_model = None
        self.original_tokenizer = None
        self.models_loaded = False

    def load_models(self):
        """åŠ è½½æ¨¡å‹"""
        if self.models_loaded:
            return "âœ… æ¨¡å‹å·²åŠ è½½"

        try:
            print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

            # åŠ è½½å¾®è°ƒæ¨¡å‹
            tuned_path = "./output/alpaca_zh_model"
            self.tuned_tokenizer = AutoTokenizer.from_pretrained(
                tuned_path, trust_remote_code=True
            )
            self.tuned_model = AutoModelForCausalLM.from_pretrained(
                tuned_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )

            # åŠ è½½åŸå§‹æ¨¡å‹ç”¨äºå¯¹æ¯”
            original_path = "Qwen/Qwen2.5-1.5B"
            self.original_tokenizer = AutoTokenizer.from_pretrained(
                original_path, trust_remote_code=True
            )
            self.original_model = AutoModelForCausalLM.from_pretrained(
                original_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )

            self.models_loaded = True
            print("æ¨¡å‹åŠ è½½å®Œæˆ!")
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹æé—®äº†"

        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

    def generate_response(self, model, tokenizer, prompt, max_length=512):
        """ç”Ÿæˆå›ç­”"""
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding=True,
            return_attention_mask=True
        )

        input_ids = inputs.input_ids.cuda()
        attention_mask = inputs.attention_mask.cuda()

        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if response.startswith(prompt):
            response = response[len(prompt):].strip()

        return response

    def compare_models(self, question):
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”"""
        if not self.models_loaded:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹", "è¯·å…ˆåŠ è½½æ¨¡å‹", "0.0", "0.0"

        prompt = f"é—®é¢˜ï¼š{question}\nå›ç­”ï¼š"

        # ç”ŸæˆåŸå§‹æ¨¡å‹å›ç­”
        start_time = time.time()
        original_response = self.generate_response(
            self.original_model, self.original_tokenizer, prompt
        )
        original_time = time.time() - start_time

        # ç”Ÿæˆå¾®è°ƒæ¨¡å‹å›ç­”
        start_time = time.time()
        tuned_response = self.generate_response(
            self.tuned_model, self.tuned_tokenizer, prompt
        )
        tuned_time = time.time() - start_time

        return original_response, tuned_response, f"{original_time:.2f}s", f"{tuned_time:.2f}s"


# åˆ›å»ºæ¨¡å‹æ¼”ç¤ºå®ä¾‹
demo = ModelDemo()


def create_demo_interface():
    """åˆ›å»ºGradioæ¼”ç¤ºç•Œé¢"""

    # é¢„è®¾é—®é¢˜
    preset_questions = [
        "è¯¦ç»†è§£é‡ŠTransformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶",
        "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
        "ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿå¦‚ä½•é¿å…ï¼Ÿ",
        "æ¯”è¾ƒCNNå’ŒRNNåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¼˜ç¼ºç‚¹",
        "å¦‚ä½•è¯„ä¼°ä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„è´¨é‡ï¼Ÿ",
        "è§£é‡Šæ¢¯åº¦ä¸‹é™ç®—æ³•çš„å·¥ä½œåŸç†"
    ]

    with gr.Blocks(
            title="Qwenå¾®è°ƒæ¨¡å‹æ¼”ç¤º",
            theme=gr.themes.Soft(),
            css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .response-box {
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 16px;
            margin: 8px 0;
            background: white;
        }
        .original-response {
            border-left: 4px solid #ff6b6b;
        }
        .tuned-response {
            border-left: 4px solid #4ecdc4;
        }
        """
    ) as demo_interface:

        gr.Markdown("""
        # ğŸš€ Qwen2.5-1.5B å¾®è°ƒæ¨¡å‹æ¼”ç¤º
        **å¯¹æ¯”å±•ç¤ºå¾®è°ƒå‰åçš„æ¨¡å‹è¡¨ç°**
        """)

        # æ¨¡å‹åŠ è½½çŠ¶æ€
        with gr.Row():
            load_status = gr.Textbox(
                label="æ¨¡å‹çŠ¶æ€",
                value="ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®åŠ è½½æ¨¡å‹",
                interactive=False
            )
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")

        gr.Markdown("---")

        # é—®é¢˜è¾“å…¥åŒºåŸŸ
        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(
                    label="ğŸ’¬ è¾“å…¥æ‚¨çš„é—®é¢˜",
                    placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨æƒ³é—®çš„é—®é¢˜...",
                    lines=3,
                    max_lines=6
                )

                submit_btn = gr.Button("ğŸš€ å¼€å§‹å¯¹æ¯”", variant="primary", size="lg")

            with gr.Column(scale=1):
                gr.Markdown("### ğŸ’¡ å¿«é€Ÿæµ‹è¯•")
                preset_btns = []
                for i, question in enumerate(preset_questions):
                    btn = gr.Button(
                        question[:30] + "..." if len(question) > 30 else question,
                        size="sm"
                    )
                    preset_btns.append(btn)

        gr.Markdown("---")

        # ç»“æœæ˜¾ç¤ºåŒºåŸŸ
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸ”µ åŸå§‹æ¨¡å‹å›ç­”")
                original_time = gr.Textbox(label="ç”Ÿæˆæ—¶é—´", interactive=False)
                original_output = gr.Textbox(
                    label="",
                    lines=8,
                    max_lines=12,
                    show_copy_button=True
                )

            with gr.Column():
                gr.Markdown("### ğŸŸ¢ å¾®è°ƒæ¨¡å‹å›ç­”")
                tuned_time = gr.Textbox(label="ç”Ÿæˆæ—¶é—´", interactive=False)
                tuned_output = gr.Textbox(
                    label="",
                    lines=8,
                    max_lines=12,
                    show_copy_button=True
                )

        gr.Markdown("---")

        # è¯„ä¼°åé¦ˆåŒºåŸŸ
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸ“Š æ”¹è¿›è¯„ä¼°")
                improvement_feedback = gr.Textbox(
                    label="æ”¹è¿›ç‚¹åˆ†æ",
                    lines=3,
                    interactive=False,
                    value="å›ç­”ç”Ÿæˆåå°†æ˜¾ç¤ºæ”¹è¿›åˆ†æ..."
                )

            with gr.Column():
                gr.Markdown("### ğŸ¯ å±•ç¤ºå»ºè®®")
                demo_tips = gr.Textbox(
                    label="ç°åœºå±•ç¤ºè¦ç‚¹",
                    lines=3,
                    interactive=False,
                    value="1. æ³¨æ„æŠ€æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§\n2. è§‚å¯Ÿå›ç­”ç»“æ„çš„å®Œæ•´æ€§\n3. æ¯”è¾ƒå®ç”¨å»ºè®®çš„ä¸°å¯Œç¨‹åº¦"
                )

        # äº‹ä»¶å¤„ç†
        def load_models_wrapper():
            return demo.load_models()

        def process_question(question):
            original, tuned, o_time, t_time = demo.compare_models(question)

            # ç®€å•çš„æ”¹è¿›åˆ†æ
            improvement = analyze_improvement(original, tuned)

            return original, tuned, o_time, t_time, improvement

        def preset_question_wrapper(question):
            return question, "", "", "", "", "ç‚¹å‡»'å¼€å§‹å¯¹æ¯”'æŸ¥çœ‹ç»“æœ"

        # ç»‘å®šäº‹ä»¶
        load_btn.click(
            load_models_wrapper,
            outputs=load_status
        )

        submit_btn.click(
            process_question,
            inputs=question_input,
            outputs=[
                original_output, tuned_output,
                original_time, tuned_time,
                improvement_feedback
            ]
        )

        for btn in preset_btns:
            btn.click(
                lambda x=btn.value: preset_question_wrapper(x),
                outputs=[
                    question_input, original_output, tuned_output,
                    original_time, tuned_time, improvement_feedback
                ]
            )

    return demo_interface


def analyze_improvement(original, tuned):
    """åˆ†ææ”¹è¿›ç‚¹"""
    improvements = []

    # é•¿åº¦å¯¹æ¯”
    if len(tuned) > len(original) * 1.5:
        improvements.append("ğŸ“ˆ å›ç­”æ›´è¯¦ç»†ä¸°å¯Œ")
    elif len(tuned) < len(original) * 0.7:
        improvements.append("ğŸ“ å›ç­”æ›´ç®€æ´ç²¾å‡†")

    # æŠ€æœ¯æ·±åº¦
    tech_terms = ['åŸç†', 'æœºåˆ¶', 'æ¶æ„', 'ç®—æ³•', 'å®ç°', 'æ­¥éª¤']
    tuned_tech = sum(1 for term in tech_terms if term in tuned)
    original_tech = sum(1 for term in tech_terms if term in original)

    if tuned_tech > original_tech:
        improvements.append("ğŸ”¬ æŠ€æœ¯æè¿°æ›´æ·±å…¥")

    # ç»“æ„å®Œæ•´æ€§
    if tuned.count('ã€‚') > original.count('ã€‚') + 2:
        improvements.append("ğŸ“‹ ç»“æ„æ›´æ¸…æ™°å®Œæ•´")

    # å®ç”¨æ€§
    practice_terms = ['ä¾‹å¦‚', 'æ¯”å¦‚', 'å…·ä½“æ¥è¯´', 'æ­¥éª¤', 'æ–¹æ³•']
    if any(term in tuned and term not in original for term in practice_terms):
        improvements.append("ğŸ’¡ åŒ…å«æ›´å¤šå®ç”¨ç¤ºä¾‹")

    if not improvements:
        improvements.append("â³ æ­£åœ¨åˆ†ææ”¹è¿›ç‚¹...")

    return " | ".join(improvements)


# åˆ›å»ºç•Œé¢
if __name__ == "__main__":
    interface = create_demo_interface()

    # å°è¯•å¤šä¸ªç«¯å£
    ports_to_try = [7861, 7862, 7863, 7864, 7865]

    for port in ports_to_try:
        try:
            print(f"å°è¯•åœ¨ç«¯å£ {port} å¯åŠ¨...")
            interface.launch(
                server_name="0.0.0.0",
                server_port=port,
                share=True,
                inbrowser=True
            )
            break
        except OSError as e:
            if "Cannot find empty port" in str(e):
                print(f"ç«¯å£ {port} ä¹Ÿè¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
                continue
            else:
                raise e
    else:
        print("æ‰€æœ‰å°è¯•çš„ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šç«¯å£")
        interface.launch(
            server_name="0.0.0.0",
            server_port=0,  # è‡ªåŠ¨é€‰æ‹©ç«¯å£
            share=True,
            inbrowser=True
        )