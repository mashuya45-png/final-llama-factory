# Model arguments
model_name_or_path: Qwen/Qwen2.5-1.5B
template: qwen

# Data training arguments
dataset: identity  # 先用内置数据集测试，稍后换成你的数据
cutoff_len: 1024
overwrite_cache: true

# Training arguments
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
output_dir: ./output/my_course_sft
overwrite_output_dir: true

per_device_train_batch_size: 2
gradient_accumulation_steps: 4
lr_scheduler_type: cosine
logging_steps: 10
save_steps: 500
learning_rate: 1.0e-4
num_train_epochs: 3.0
max_samples: 1000
max_steps: 5  # 先用5步测试

# FP16/BF16 training
fp16: true

# Evaluation arguments
do_eval: false
